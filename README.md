# COMP61021 – Representation Learning | Coursework 2: Variational Autoencoders (VAE)

**University of Manchester** | Student ID: M63556AB

---

## Overview

This repository contains the implementation and analysis for Coursework 2 of COMP61021 – Representation Learning. The assignment focuses on **Variational Autoencoders (VAEs)** built with PyTorch, applied to the (Fashion-)MNIST dataset.

The VAE consists of:
- An **Encoder** network that maps input images to a latent distribution (mean + log-variance)
- A **Decoder** network that reconstructs images from latent samples
- A training objective minimising the **Variational Free Energy (VFE)** = Reconstruction Loss + KL Divergence

---

## Repository Structure

```
.
├── 2023_VAE_PyTorch.ipynb          # Original base VAE notebook (provided)
├── 2023_VAE_PyTorch copy.ipynb     # Intermediate working notebook
├── 2023_VAE_PyTorch copy 2.ipynb   # Extended experiments (Q4–Q7 work)
├── M63556AB_RepLearning.ipynb      # Main submission notebook
├── generated_sample.png            # Sample images generated by trained decoder
└── README.md
```

---

## Assignment Questions

### Q4 – Hyperparameter Grid Search over Hidden & Latent Dimensions

Train the VAE across a grid of hyperparameter combinations and report the best test Variational Free Energy (VFE) for each:

- **Hidden dimensions:** `[5, 50, 200, 500]`
- **Latent dimensions:** `[5, 10, 100, 300]`

Submit a properly labelled **heatmap** of the test VFE values across the grid of `(hidden_dim, latent_dim)` combinations.

---

### Q5 – Larger Architecture Grid Search

Repeat the VFE grid search experiment from Q4 with a larger set of architecture sizes:

- **Hidden dimensions:** `[128, 256, 512, 1024]` (i.e., `2^7, 2^8, 2^9, 2^10`)
- **Latent dimensions:** `[2, 4, 8, 16, 32, 64, 128, 256]` (i.e., `2^1` through `2^8`)

Submit a properly labelled heatmap of the test VFE values for this extended grid.

---

### Q6 – Effect of Weight Decay (Regularisation)

Investigate the effect of **L2 regularisation (weight decay)** on the VAE's test VFE. Re-run the Q4 grid search experiments with a non-zero weight decay parameter added to the Adam optimiser.

Submit a heatmap of test VFE values analogous to Q4, showing how regularisation affects the results.

---

### Q7 – VFE vs Total Number of Parameters (Model Complexity Curve)

Re-interpret the Q4 experiments in terms of **model complexity**. For each `(hidden_dim, latent_dim)` combination, compute the total number of trainable parameters in the VAE (encoder + decoder). Then:

- Plot **test VFE vs total number of parameters**
- Draw a **vertical line** at the point on the x-axis corresponding to the **total training data size** (60,000 × 784 = 47,040,000 values for MNIST)
- Ensure a couple of model sizes are explored on each side of this line

This plot illustrates the classical bias-variance tradeoff in the context of deep generative models.

---

## Key Concepts

| Term | Description |
|---|---|
| **VAE** | Variational Autoencoder — a generative model that learns a latent representation |
| **VFE** | Variational Free Energy = Reconstruction Loss + KL Divergence |
| **KL Divergence** | Measures how much the learned latent distribution deviates from the prior N(0, I) |
| **Reparameterisation** | Trick to allow backpropagation through stochastic sampling |
| **Latent Dimension** | Dimensionality of the compressed latent space z |

---

## Requirements

Install dependencies with:

```bash
pip install torch torchvision matplotlib seaborn tqdm
```

> If running on a system without `torchvision` pre-installed:
> ```bash
> source /etc/anaconda3/bin/activate
> pip install torchvision
> ```

---

## Results

The trained decoder successfully generates realistic (Fashion-)MNIST images from pure Gaussian noise, demonstrating that the VAE has learnt a meaningful latent space.

Sample generated images:

![Generated Samples](generated_sample.png)

---

## References

- [PyTorch VAE Tutorial (Jackson-Kang)](https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/01_Variational_AutoEncoder.ipynb)
- [Probability Course – Conditional Probability](https://www.probabilitycourse.com/chapter1/1_4_0_conditional_probability.php)
- [Madhur Tulsiani – Information Theory Notes](https://home.ttic.edu/~madhurt/courses/infotheory2021/)
- [Lazebnik – Variational Autoencoders (Slides)](https://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf)
